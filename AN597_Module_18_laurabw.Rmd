---
title: "Module 18"
author: "Laura Brubaker-Wittman"
date: "11/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Mixed Model Effects

First, make sure all packages needed are loaded:
```{r}
library(curl)
library(ggplot2)
library(lme4)
library(AICcmodavg)
library(MuMIn)
```

## Objectives:

In this module, we extend our discussion of regression modeling even further to include mixed effects models.

## Mixed Effects Models

A final extension of linear regression modeling that we will talk about is so-called “multilevel” or “mixed
effects” modeling. This is a very complex topic, and we will only scratch the surface! There are many varieties
of mixed models.

- **Linear mixed models (LMM)**, if we’re dealing with normally distributed variables and error structures.
- **Generalized linear mixed models (GLMM)**, if we’re dealing with various other variable types and error
  structure (e.g., binary, proportion, or count data).
- **Nonlinear mixed models (NLMM)**, if we’re dealing with situations where our response variable is best
modeled by a nonliner combination of predictors.

In a (general or generalized) linear mixed model, we have a reponse variable, ***Y***, and observations that
fall into different factor categories each with some set of levels (e.g., “sex” with levels “male” and
“female”), and we are interested in the effects of these factors and factor levels on the response variable.
Generically, if ***μ*** = a population mean response and ***μA*** = mean response for observations belonging to
factor level A, then the effect of A is given by ***μ - μA***. We have already dealt with factors and factor
levels in our linear regression models when we looked at categorical predictors (e.g., *sex, rank category*) in
our discussions of ANOVA and ANCOVA.

We can conceptualize factor effects as being either **fixed** or **random**. **Fixed** factors are those that
reflect all levels of interest in our study, while random effects are those that represent only a sample of the
levels of interest. For example, if we include sex as a factor in a model with the factor levels “male” and
“female”, this (typically) will cover the gamut of levels of interest our study, thus we would consider sex a
fixed factor. When we were doing ANOVA and ANCOVA analyses previously, we were looking at the effects of such
fixed factors.

However, if our observational data were to consist of repeated observations of the same sampling unit, e.g.,
measurements taken on the same set of individuals on different dates, individual ID would be considered a random
factor because it is unlikely that we will have collected data from all possible “levels of interest”, i.e.,
from all possible individual subjects. We have not yet dealt with such random factors as an additional source of
variance in our modeling.

**Mixed models** are those that include BOTH fixed and random effects. Including random effects in addition to fixed
effects in our models has several ramifications:

- Using random effects broadens the scope of inference. That is, we can use statistical methods to infer something about the population from which the levels of the random factor have been drawn.

- Using random effects naturally incorporates dependence in the model and helps us account for pseudoreplication
in our dataset. Observations that share the same level of the random effects are explicitly modeled as being
correlated. This makes mixed effect modeling very useful for dealing with time series data, spatially correlated
data, or situations where we have repeated observations/measures from the same subjects or sampling unit.

- Using random factors often gives more accurate parameter estimates.

- Incorporating random factors, however, does require the use of more sophisticated estimation and fitting
methods.

We will explore mixed effects modeling using an example based on this excellent tutorial:
https://arxiv.org/ftp/arxiv/papers/1308/1308.5499.pdf

## Example about sexual swellings in chimpanzees:

Suppose we have measured the amount of grooming received by female chimpanzees when they are either in their
periovulatory period (i.e., the window of 2-3 days around the likely time of ovulation) or during other portions
of their reproductive cycle. We collected data on the *duration of grooming bouts received* and scored the
female’s *reproductive condition* at the time as a categorical factor with two levels: “POP” versus “NONPOP”. On
top of that, we also recorded data on female parity at the time of the grooming bout, i.e., whether the female
had given birth previously (was “parous”, or “P”) or had not yet had an offspring (was “nulliparous”, or “N”).

If we’re interested in how reproductive condition and parity influence how much grooming a female receives, our
regression model would look like this:

*grooming duration ~ condition + parity + ϵ*

Also imagine that our study design was such that we took multiple observations per subject. That is, our data
set includes records of multiple grooming bouts received by each subject. This situation violates the
assumption of independence of observations that we make for standard linear regression: multiple
responses/measures from the same subject cannot be regarded as independent from each other.

Using a mixed effects model, we can deal with this situation by adding **subject ID** as a random effect in our
model. Doing so allows us to address the nonindependence issue by estimating a different set of parameters for
each level of the factor subject. We can either estimate a different intercept for each subject (which would
correspond to each female having a different “baseline” level of grooming received) or estimate a different
*intercept* **and** *slope* (where individual subjects are presumeed to differ both in the baseline level of
grooming received and the strength of the relationship between grooming duration, on the one hand, and
reproductive condition and parity, on the other). Our mixed effects model estimates these individual level
parameters in addition to the main effects of each variable.

This is why a mixed effects model is called a **mixed** model. The models that we have considered so far have
been “fixed effects only” models and included only one or more “fixed” predictor variables and a general error
term. We essentially divided the world into things that we somehow understand or that are systematic (the fixed
effects, or the explanatory variables) and things that we could not control for or do not understand (ϵ). These
fixed effects models did not examine possible structure within the error term. In a mixed model, by contrast,
we add one or more random effects to our fixed effects that may explain a portion of the variance in our error
term.

**CHALLENGE 1**

Let’s explore these idea using some actual data. First, load in the dataset “chimpgrooming.csv” and do some
exploratory data analysis:

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/chimpgrooming.csv")
d <- read.csv(f, header = TRUE, sep = ",")
head(d)
summary(d)
```

Now, let's make a box plot looking at individual chimp ladies:

```{r}
# first, some exploratory visualization let's plot grooming received
# duration in relation to subject ID
par(mfrow = c(1, 1))
boxplot(data = d, duration ~ subject, col = c("lightpink1"))
```

Now let's plot looking at POP vs. non-POP:

```{r}
# we see lots of individual variation let's plot grooming received duration
# in relation to reproductive condition
boxplot(data = d, duration ~ reprocondition, col = c("burlywood2", "lightpink1"))
```

Now, we can see if there is an interaction effect, that takes into account both parous condition and reproductive state:

```{r}
# let's plot grooming received duration in relation to reproductive
# condition and parity
boxplot(data = d, duration ~ reprocondition * parity, col = c("burlywood2", 
    "lightpink1"))
```

Now, let's look at individuals in their different reproductive states:

```{r}
boxplot(data = d, duration ~ reprocondition * subject, col = c("burlywood2", 
    "lightpink1"))
```

*What patterns do we see?*

RANDOM INTERCEPT MODELS

We will now perform an initial mixed effects analysis where we look at how reproductive condition and parity (as fixed effect) effect grooming duration, where we include individual subject ID as a random effect.

Here is a first mixed effects model that we will fit, using one extension of formula notation that is commonly used in ***R***.

*grooming duration ~ condition + parity + (1|subject) + ϵ*

Here, the 1 refers to the fact that we want to estimate an intercept and the pipe “|” operator following the “1” signifies that we want to estimate a different intercept for each subject. Note that this generic formula still contains a general error term, ϵ, to highlight that there will still be unexplained “error” variance after accounting for both fixed and random effects in the model.

We can think of this formula as saying that we expect our dataset to include multiple observations of the response variable per subject, and these responses will depend, in part, on each subject’s baseline level. This effectively accounts the nonindependence that stems from having multiple responses by the same subject.

The {lme4} package in ***R*** is commonly used for mixed effects modeling, and the function lmer() is the mixed model equivalent of the function lm(). In the formula syntax for mixed effects models using the {lme4} package, fixed effect are included without parentheses while random effects are included in parentheses (the error, ϵ, is understood and is not included explicitly).

*NOTE:  We could also use the package {nlme} for mixed effects modeling (which requires a slightly different formula syntax that that used here). Both packages also allow us to do GLMMs and nonlinear mixed effects modeling, which we will not be discussing. It is important to note that {lme4} uses, by default, a slightly different parameter estimation algorithm than {nlme}. Unless otherwise specified, {lme4} uses “restricted maximum likelihood” (REML) rather than ordinary maximum likelihood estimation, which is what is used in {nlme}. In practice, these give very similar results. We will see below that when we want to compare different models using {lme4}, we will need to tell {lme4} to use ordinary maximum likelihood.*

The code below shows how to implement this first **mixed effects model**:

```{r}
lme <- lmer(data = d, duration ~ reprocondition + parity + (1 | subject))
summary(lme)
```

Let’s focus on the output for the random effects first. Have a look at the column standard deviation. The entry for *subject* shows us how much variability in grooming duration (apart from that explained by the fixed effects) is due to subject ID. The entry for *Residual* summarizes the remaining variability in grooming duration that is not due to *subject* nor to our fixed effects. This is our ϵ, the “random” deviations from the predicted values that are not due to either subject or our fixed effects.

The fixed effects output mirrors the coefficient tables that we have seen previously in our linear models that have focused only on fixed effects. The coefficient “reproconditionPOP” is the *β* coefficient (slope) for the categorical effect of reproductive condition. The positive for the coefficient means that grooming duration is GREATER by 20.293 units for POP than for NONPOP females. Then, there’s a standard error associated with this slope, and a t value, which is simply the estimate divided by the standard error.

The coefficient “parityP” is the *β* coefficient for the categorical effect of parity. The grooming duration associated with being parous versus nulliparous is GREATER by 109.65 units.

The INTERCEPT in this case is the grooming duration associated with nulliparous, NONPOP females. Like the lm() function, the lmer() took whatever factor level came first in the alphabet to be the reference level for each fixed effect variable.

Let’s also look at the coefficients coming out of the model. 
